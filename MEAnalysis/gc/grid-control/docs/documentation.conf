; ==============================================================================
; General options
; ==============================================================================

[DEFAULT]
dir = .

[global]
task          = CMSSW         ; Available options: 
                              ; CMSSW, UserTask, ROOTTask
                              ; |ADVANCED USAGE: Classes can be specified in different ways:
                              ; | * grid_control.user_mod.UserTask (fully qualified path)
                              ; | * user_mod.UserTask (lookup in grid_control is default)
                              ; | * UserTask (short form in case of import by __init__.py)
backend       = grid          ; Available options: [grid], local
workdir       = %(dir)s/work  ; Location of the work directory - default: Name of config file
workdir space = 50            ; Lower space limit in work directory, deactivate with 0 - default: 10
include       = common.conf   ; List of additional config files which provide default values.
                              ; These config files are processed in addition to the files:
                              ; /etc/grid-control.conf, ~/.grid-control.conf and <GCDIR>/default.conf
cmdargs       = -G -c         ; Automatically added command line arguments - default: empty
                              ; Here, -G -c enables the GUI and continuous mode

[jobs]
jobs          = 27            ; Maximum number of jobs (truncated to task maximum)
                              ; Default is taken from task maximum
in flight     = 10            ; Maximum number of concurrently submitted jobs - default: no limit
in queue      = -1            ; Maximum number of queued jobs - default: no limit
max retry     = 4             ; Number of resubmission attempts for failed jobs - default: no limit
continuous    = True          ; Enable continuous mode - default: False
action        = check submit  ; Specify the actions and the order in which grid-control should perform them
                              ; Default: check, retrieve, submit

cpus          = 1             ; Requested number of cpus per node - default: 1
memory        = 512           ; Requested memory in MB - default: 512
                              ; NAF jobs need 2000 MB !
wall time     = 10:00:00      ; Requested wall time in format hh[:mm[:ss]]
                              ; also used for checking the proxy lifetime
cpu time      = 10:00         ; Requested cpu time in format hh[:mm[:ss]] - default: wall time

queue timeout = 2:00:00       ; Resubmit jobs after staying some time in initial state - default: off
node timeout  = 0:10:00       ; Cancel job after some time on worker node - default: off
monitor       = dashboard     ; Monitoring of jobs (can be combined like: "dashboard,scripts")
                              ;    dashboard - use dashboard
                              ;    scripts   - call scripts (see [events] section) [default]
shuffle       = True          ; Submit jobs in random order - default: False
nseeds        = 20            ; Number of random seeds to generate - default: 10
seeds         = 32 51 346 234 ; Random seeds used in the job via @SEED_j@
                              ; @SEED_0@ = 32, 33, 34, ... for first, second, third job
                              ; @SEED_1@ = 51, 52, 53, ... for first, second, third job
                              ; Default: Generate <nseeds> random seeds
selected      = var:KEY=VALUE ; Apply general job selector - default: none
kick offender = 4             ; Threshold for dropping jobs causing status retrieval errors - default: 10

[events]
silent        = True          ; Do not show output of event scripts - default: true
; There are many variables set for these scripts according to the
; environment variables the job will have during execution - they are prefixed "GC_"
; note: the events are only evaluated with "monitor = scripts" in [jobs] section
; In addition to the normal job variables, the following variables are always defined:
;   CFGFILE = absolute path to config file
;   WORKDIR = absolute path to working directory
on submit     = on_submit.sh @CFGFILE@ @JOBNUM@
; To next event has access to the variable "STATUS":
on status     = on_status.sh @NICK@ @STATUS@
; The following event has the additional variable RETCODE (= return code of job)
on output     = on_output.sh @WORKDIR@ @RETCODE@
; For the following event only job invariant constants and the
; variable NJOBS (= number of jobs) are available
on finish     = on_finish.sh @TASK_ID@ @NJOBS@

[dashboard]
application   = Herwigpp      ; Defines application name - default: shellscript
task          = production    ; Task type reported to the dashboard - default: analysis
task name     = @TASK_ID@     ; Taskname in dashboard - default: @TASK_ID@_@NICK@
                              ; ("_" are stripped away from the edges)

; ==============================================================================
; Backend options
; ==============================================================================

[local]
proxy         = TrivialProxy  ; Available options: [TrivialProxy], VomsProxy
check storage = False         ; Check storage requirements before submission - default: True
                              ;   False: Submit jobs even with empty storage requirements
wms           = PBS           ; Select local wms: PBS, LSF, SGE, SLURM, Host - default: best guess
sites         = -node003 nd01 ; Whitelist / Blacklist nodes (prefix "-")
queue         = short long    ; User specified list of local queues
broker        = DummyBroker   ; Available options: [DummyBroker], SimpleBroker
delay output  = False         ; Delay job output / error streaming to end of job - default: False

sandbox path  = %(dir)s/sbox  ; Path to sandboxes - default: $WORKDIR/sandbox
scratch path  = /tmp          ; Override scratch directory on nodes - default: determined by job
server        = servername    ; Name of batch server - default: None
group         = cmsqcd        ; Select local fairshare group

[PBS]
attribute     = value1        ; Forward options to batch system

[grid]
proxy         = VomsProxy     ; Available options: TrivialProxy, [VomsProxy]
wms           = GliteWMS      ; Available options: [GliteWMS], Glite, LCG
sites         = -ce.grid.net  ; Whitelist / Blacklist sites (prefix "-")
vo            = cms           ; Select vo - default: provided by proxy

[proxy]
min lifetime    = 10:00       ; Minimal required lifetime of the proxy - default: 5:00
ignore warnings = True        ; Ignore problems with the proxy as long as all
                              ; necessary information is provided - default: False

[glite-wms]
use delegate  = False         ; Use delegation proxy for job submissin - default: True
ce            = ce1.gridka.de ; Select destination CE
discover wms  = True          ; Discover available WMS - default: True
discover sites = True         ; Discover available sites - default: False
;config = docs/glite_wms_ALL.conf ; GliteWMS backend specific configuration (WMS, ...)

; ==============================================================================
; Storage options
; ==============================================================================

[storage]
; se path specifies the default location(s) used to transfer "se input files" at the
; beginning of the job and "se output files" at the end of the job
; Currently supported protocols: gsiftp srm rfio dir
se path       = gsiftp://ekp-lcg-se.physik.uni-karlsruhe.de//wlcg/data/users/cms/my_username
;se path      = srm://dcache-se-cms.desy.de:8443/pnfs/desy.de/cms/tier2/store/user/
;se path      = srm://dcache-se-cms.desy.de:8443//srm/managerv2?SFN=/pnfs/desy.de/cms/tier2/store/user/
;se path      = rfio:///castor/cern.ch/user/x/username
;se path      = dir:///absolute/path/to/directory

se min size       = -1                  ; Job fails if any output file is smaller than se min size
se output path    = dir:///storage/     ; Location to save se output files - default: <se path>
se output files   = out.root            ; Specifies the files to be transfered after the job has finished
se output pattern = job_@MY_JOBID@_@X@  ; This pattern is applied to the se output file
                                        ; to get the destination filename
                                        ; Default: @NICK@job_@MY_JOBID@_@X@
                                        ; @X@         : Marks the spot of the original filename
                                        ; @XBASE@     : Original filename without extension
                                        ; @XEXT@      : Only the extension of the original filename
                                        ; @MY_JOBID@  : Continous job number 0..max
                                        ; @NICK@      : Nickname of dataset for CMSSW jobs
                                        ; @CONF@      : Name of config file (without extension .conf)
                                        ; @DATE@      : Current date eg. 2009-06-24
                                        ; @TIMESTAMP@ : Current timestamp eg. 1245855548
                                        ; @RANDOM@    : Random number between 0 and 900000000
                                        ; This is just the list of the most important substituions available
                                        ; A complete list is available via the --help-vars option
                                        ; The variables can also be surounded by "__" instead of "@"

se input path     = dir:///storage/     ; Location with se input files - default: <se path>
se input files    = file                ; Specifies the files to be transfered before the job starts
se input pattern  = @X@                 ; This pattern is applied to the se input file
                                        ; to get the source filename. Same rules as output pattern
                                        ; Default: @X@
se input timeout = 00:30                ; timeout for the transfer of se input files to the worker node
                                        ; (includes runtime, if 'SE runtime' is True). Default: 0:10 (10 minutes)
se output timeout = 2:00                ; timeout for the transfer of se output files. Default: 1:00 (1 hour)

; During the duration of the job both the available and used space is monitored
; The following entries specify thresholds (in mb) which cause the job to abort
; Landing zone is the directory the job initially starts in
landing zone space used = 100           ; Maximum amount of space used by the job
                                        ; Default: 100 mb
landing zone space left = 50            ; Minimum amout of disk space available
                                        ; Default: 1 mb
; One of the first orders of business for each job is to find a large
; scratch space which will be used as working directory of the job
; If the landing zone ITSELF is the scratch space, the scratch thresholds apply
scratch space used = 5000               ; Maximum amount of space used by the job
                                        ; Default: 5000 mb
scratch space left = 1000               ; Minimum amout of disk space available
                                        ; Default: 1 mb

; ==============================================================================
; User Task
; ==============================================================================

[UserTask]
send executable = False                 ; Put executable into the input sandbox - default: True
executable   = default.sh               ; Name of the script / application
arguments    = param1 param2 param3     ; Parameters for the user application
                                        ; Known variables in the form @VAR@ will be replaced.
                                        ; A complete list is available via the --help-vars option
input files  = input.txt config.sh      ; Input files send together with the job
                                        ; Only for small files - send large files via SE!
subst files  = config.sh                ; These input files will be subjected to variable substituion
                                        ; A complete list is available via the --help-vars option
output files = output.gz                ; Output files retrived together with the job
                                        ; Only for small files - send large files via SE!
depends      = CMSSW                    ; Depends on certain environments eg. Herwig++ job which needs
                                        ; CMSSW. Possible values: CMSSW, gLite
                                        ; The corresponding files are share/env.cmssw.sh, share/env.glite.sh
constants    = CONST1 CONST2            ; Define additional constants used in variable substitution:
CONST1       = value 1                  ; @CONST1@ = "value 1"
CONST2       = 1234567                  ; @CONST2@ = "1234567"

; ==============================================================================
; ROOT Task
; ==============================================================================
[ROOTTask]
; Uses exactly the same options as UserTask
root path    = /opt/root                ; Path to ROOT installation


; ==============================================================================
; CMSSW Task
; ==============================================================================

[CMSSW]
project area     = %(dir)s/CMSSW_3_1_0  ; Path to an existing CMSSW project area used for running the jobs
;scram project   = CMSSW CMSSW_3_1_0    ; Used to create a vanilla CMSSW project, eg. for production
;scram arch       = slc4_ia32_gcc345    ; Select scram architecture
                                        ; When given a project area, the default arch is taken from the project
                                        ; Has to be specified when using scram project
cmssw dir        = /path/for/VO_CMS_SW_DIR ; Path to manually add cmssw dir to search path

; Path to CMSSW config file 
config file      = %(project area)s/src/Test/Analysis/cmssw-grid.py
prepare config   = True                 ; Append fragment for CMSSW all config files - default: False
instrumentation fragment = fragment.py  ; Specifies path to fragment - default: <...>/fragmentForCMSSW.py

software requirements    = True         ; Write CMSSW version into job requirements - default: True
gzip output      = True                 ; Gzip the output of the cmsRun command - default: True
se runtime       = True                 ; Send CMSSW runtime via SE instead of sending it together with the job - default: False
se runtime force = True                 ; Force to overwrite existing se runtimes - default: True

; Comment out the variable [jobs] jobs in order to run over all events of the dataset
; Specifiy one dataset 
;dataset = /WmunuJets_pt_80_120/CMSSW_1_3_1-Spring07-1243/GEN-SIM-DIGI-RECO
; Or several by starting with an empty line
dataset =
	/WmunuJets_pt_80_120/CMSSW_1_3_1-Spring07-1243/GEN-SIM-DIGI-RECO#2c1efdb8-d9ba-46d4-b067-72c3d8b19abf
	QCD : /QCD_Pt_470_600/CMSSW_1_5_2-CSA07-2096/GEN-SIM-DIGI-RECO@cms_dbs_prod_local_09
	/CSA07JetMET/CMSSW_1_6_7-CSA07-Tier0-A1-Gumbo/RECO
	Zmm : DBS:/Zmumu/Summer08_IDEAL_V9_v1/GEN-SIM-RAW
;	Nick1 : list:/path/to/local/dbsfile
;	Nick2 : file:/pnfs/to/file|1200
; dataset syntax:
;     [<nickname> : [<protocol> :]] <dataset specifier>
;     Syntax for the dataset specifier depends on the protocol:
;          dbs : <dataset path>[@<instance>][#<block>]
;         list : <path to list of data files>[@<forced prefix>][%<selected dataset>[#<selected block>]]
;                The list provider allows to select single datasets / blocks from a file
;         file : <path to data file>|<number of events>[@SE1,SE2]

; Select default dataset protocol
; Available: [DBSApiv2] (=dbs) FileProvider (=file) ListProvider (=list)
dataset provider = DBSApiv2
dataset refresh  = 4:00       ; Refresh dataset information every 4h
dataset storage check = False ; Check storage requirements before submission - default: True
                              ;   False: Submit jobs even with empty storage requirements

dataset splitter = EventBoundarySplitter ; Available options:
                                         ; EventBoundarySplitter: splits on block boundaries and slices each block into
                                         ;                        jobs with "events per job" events per job
                                         ;                        (Default for CMSSW jobs)
                                         ; HybridSplitter:        hybrid of EventBoundarySplitter and FileBoundarySplitter,
                                         ;                        splits on file and block boundaries and tries to use
                                         ;                        approximately "events per job" events per job
                                         ; FileBoundarySplitter:  splits on file and block boundaries
                                         ;                        and interpret "files per job" as files per job
                                         ;                        (Default for UserTask jobs)
                                         ; BlockBoundarySplitter: split along block boundaries

events per job   = 5000                  ; Set granularity of dataset splitter
                                         ; Without datasets, this sets the variable MAX_EVENTS
files per job    = 5                     ; Set granularity of dataset splitter

lumi filter      = 132440:1-132440:401,  ; apply global luminosity block filter for all DBS datasets
                   132442-132447,        ; Syntax: <RUN>[:<LUMI>]-[<RUN>[:<LUMI>]], ...
                   132473, 132652,       ; (open ended ranges possible)
                   /path/to/json_file    ; alternative way to specify lumi sections is via json file
                   json_fileY|-12345     ; get infos from json file up to run 12345

; Select files to be included in the CMSSW runtime
; Default: -.* -config lib module */data *.xml *.sql *.cf[if] *.py
area files       = -.* -config lib module */data *.xml *.sql *.cf[if] *.py

prolog executable = default.sh          ; List of scripts / applications to execute before cmsRun
                                        ; in the prepared CMSSW area
prolog arguments  = @DATASETNICKNAME@   ; Arguments to be supplied to the prolog scripts
epilog executable = afterjob.sh         ; List of scripts / applications to execute after cmsRun
                                        ; in the prepared CMSSW area
epilog arguments  = @VAR1@ -Lsv         ; Arguments to be supplied to the epilog scripts

arguments        = param1 param2 param3 ; Arguments supplied to cmsRun
;input files     =                      ; Input files send together with the job
                                        ; Only for small files - send large files via SE!
;subst files     =                      ; These input files will be subjected to variable substituion
                                        ; A complete list is available via the --help-vars option
                                        ; Default: the CMSSW config file
;output files    =                      ; Output files retrived together with the job
                                        ; Only for small files - send large files via SE!
constants    = CONST1 CONST2            ; Define additional constants used in variable substitution:
CONST1       = value 1                  ; @CONST1@ = "value 1"
CONST2       = 1234567                  ; @CONST2@ = "1234567"

[constants]
; Define additional constants (always uppercase) to allow usage in variable substitution:
; These variables are overridden by the constants specified in the task section
; To define constants with different cases, use "constants = ..." in the section of the task
const1       = value 1                  ; @CONST1@ = "value 1"
CONST2       = 1234567                  ; @CONST2@ = "1234567"

; The dataset section contains general options for dataset providers
[dataset]
sites               = -gridka.de        ; White/Blacklist for storage location of dataset based jobs
remove empty blocks = False             ; Remove empty blocks from processing - default: true
remove empty files  = False             ; Remove empty files from processing - default: true
nickname source     = NickNameProducer  ; Name of plugin to produce nickname depending on dataset name
                                        ; and currently specified nickname - default: SimpleNickNameProducer

limit events        = 10000             ; Stop processing after 10000 events - default: -1
ignore files        = /path/file1       ; List of file names to ignore during processing
                      /path/file2       ; (broken files from dbs)
                      /path/file3
resync interactive  = True              ; Switch for interactive resync mode - default: true

resync jobs         = refill            ; Changed dataset splittings will be
                                        ;   [append] - new and changed entries will be appended
                                        ;   preserve - changed entries will stay at their position, new ones get appended
                                        ;   fillgap - new entries will be used to replace invalidated entries - holes can still exist
                                        ;   reorder - new and changed entries will replace invalidated entries

resync mode new     = ignore            ; Resync mode for new files - [complete], ignore
resync mode removed = disable           ; Resync mode for removed files - disable, [complete], ignore
resync mode expand  = complete          ; Resync mode for expanded files - disable, complete, [changed], ignore
resync mode shrink  = changed           ; Resync mode for shrunken files - disable, complete, [changed], ignore
                                        ;   disable - disable affected jobs
                                        ;   complete - all jobs with affected files need to be reprocessed
                                        ;   changed - only jobs with changes in the area are reprocessed
                                        ;   ignore - ignore changes

resync metadata     = Run, UserMD       ; List of metadata variables to consider during resync
                                        ; (All metadata variables are updated during a resync)
resync mode Run     = ignore            ; Resync mode for changes in metadata entry "Run" - disable, [complete], ignore
resync mode UserMD  = disable           ; Resync mode for changes in metadata entry "UserMD" - disable, [complete], ignore

; DBSApiv2
; ------------
; Options specific to DBSApiv2 provider:
;   Select default dbs instance for DBSApiv2 datasets
;   Default: http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet (Global CMSSW production DBS server instance)
dbs instance         = http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet
only valid           = True              ; Retrieve only data with status "VALID"
use phedex           = False             ; Switch for site info retrieval via DBS / phedex
phedex sites         = -T0_CH_CERN       ; Black/Whitelist with siteDB names for dataset locations
only complete sites  = True              ; Retrieve only complete dataset blocks

; ScanProvider
; ------------
; Options specific to ScanProvider:
dataset name pattern = Data_@TASK_ID@   ; Dataset naming scheme - default: dataset key
block name pattern   = @BLOCK_KEY@      ; Block naming scheme - default: dataset key

dataset hash keys    = VAR1 VAR2 VAR3   ; List of variables to determine dataset boundaries
block hash keys      = VARA VARB VARC   ; List of variables to determine block boundaries

dataset guard override = VAR1 VAR2      ; Override dataset boundary security measures
block guard override   = VARA VARB      ; Override block boundary security measures

; Available scanners: 
; * OutputDirsFromConfig                  (<> => <OUTPUTDIR>)
source config = /path/to/config.conf    ; Path to external config file
source job selector   = var:KEY=VALUE   ; Selects which output directories are retrieved - default: None
; * OutputDirsFromWork                    (<> => <OUTPUTDIR>)
source directory      = /path/to/output ; Path to root of output directories
; * FilesFromJobInfo                      (<OUTPUTDIR> => <FILE>)
; * FilesFromLS                           (<> => <FILE>)
source directory      = /storage/skim   ; Path to dataset files
; * FilesFromDataProvider               ; (<> => <FILE>)
source dataset path   = dbs://...       ; Take files from dataprovider (as flat list with metadata)
; * MetadataFromTask                      (<FILE> => <FILE>)
ignore task vars    = VAR1 VAR2 VAR2    ; List of ignored task variables - default: <common gc variables>
; * DetermineEvents                       (<FILE> => <FILE>)
events command        = /path/to/getEv  ; Program which returns number of events in file - default: None
                                        ; Example to retrieve number of events from EDM files:
                                        ;   function edmsize() { edmEventSize -v $1 | grep File | awk '{n = split($0,array," ")} END{print array[n]}'; }; edmsize
events key            = NUM_EVENTS      ; Variable which contains number of events
events default        = 1000            ; Number of events to return in case other methods fail - default: -1
; * MatchOnFilename                       (<FILE> => <FILE>)
filename filter       = *.root *.dat    ; List of filename patterns to match against filenames
; * MatchDelimeter                        (<FILE> => <FILE>)
delimeter match       = _:5             ; Match filenames with specified number (5) of delimeters (_) - default: None
delimeter dataset key = _:0:4           ; Substring of filename to be used to determine dataset / block
delimeter block key   = x:7             ; Format: <delim>[:<start>[:<end>]] - default: None
; * AddFilePrefix                         (<FILE> => <FILE>)
filename prefix       = file://         ; Add a prefix to each entry

; * ObjectsFromCMSSW                      (<OUTPUTDIR> => <OUTPUTDIR>)
include parent infos  = True            ; Parse parent information - default: False
merge config infos    = True            ; Merge config files according to their hash value
; * MetadataFromCMSSW                     (<FILE> => <FILE>)
include config infos  = True            ; Include CMSSW config infos - default: False
; * SEListFromPath                        (<FILE> => <FILE>)
; * LFNFromPath                           (<FILE> => <FILE>)
lfn marker            = /data/          ; Part of path to determine lfn start - default: /store/
; * FilterEDMFiles                      ; (<FILE> => <FILE>)

; List of plugins to schedule on "file bus"
scanner = OutputDirsFromConfig FilesFromJobInfo DetermineEvents MatchDelimeter

; Options for a particular dataset with nickname TESTNICK can be specified with:
[dataset TESTNICK]
sites               = -fnal.gov         ; White/Blacklist for storage location of dataset based jobs

; ==============================================================================
; CMSSW Advanced Task
; ==============================================================================
[CMSSW_Advanced]
; Uses exactly the same options as CMSSW
nickname constants  = GLOBALTAG ACTIVE  ; Name of nickname specific variables
ACTIVE              = 'Tracks'          ; Simple form for nickname variable - single value for all nicknames
GLOBALTAG           =  START3X_V26::All ; Default value in case no nickname matches
          2010APRv1 => GR10_P_V5::All   ;   <regular expression for nickname> => <variable value>
          2010APRv2 => GR10_P_V6::All

nickname config     =  skim_MC.py       ; Default CMSSW config file is skim_MC.py
          2010APRv1 => skim_C10.py      ;   <regular expression for nickname> => <config file>
          2010APRv2 => skim_C10_36x.py

nickname lumi filter =                  ; Default luminosity filter
               muPD => 135500-136500    ;   <regular expression for nickname> => <luminosity filter>
                jmt => 135000-136000,136100:10-137000:530,selection.json

; ==============================================================================
; Parameter settings
; ==============================================================================

[parameters]
parameter adapter = TrackedParameterAdapter ; Specifiy adapter between jobs and parameters
  ; BasicParameterAdapter - can be used for static parameter spaces (fast)
  ; TrackedParameterAdapter - (default) can handle changes to parameter space setup during runtime (versatile)

parameter factory = SimpleParameterFactory  ; Specify factory class to generate the parameter space
  ; BasicParameterFactory - only supports entries in [constants]
  ; SimpleParameterFactory - simple parameter syntax to describe many common parameter spaces
  ; ModularParameterFactory - advanced parameter syntax with complete access to all available functions

repeat = 10                                 ; Defines how many jobs process any given parameter space point

; ------------------------
; Variables
A = 1 2 3                               ; Simple list of parameter values
                                        ; => A = ['1', '2', '3']

B = m 1                                 ; Line separated list of parameter values
  n 2                                   ; => B = ['m 1', 'n 2']
B type = lines                          ; Parsing description for variable B

C = range(10, 13)                       ; Parameter values given by inline python expression
C type = expr                           ; => C = [10, 11, 12]

D = XX_XY_XZ                            ; User defined splitting of parameter values
D type = split                          ; => D = ['XX', 'XY', 'XZ']
D delimeter = _                         ; delimeter specification

(T1, T2) = (1 A, y1) (2 B, y2)          ; T1 and T2 are specified in tuple form
  (3 C, y3) (4 D, y4)                   ; but they can also be used independently
                                        ; => T1 = ['1 A', '2 B', '3 C', '4 D']
                                        ; => T2 = ['y1', 'y2', 'y3', 'y4']

(Bl, Bh) = 1 2 3 4                      ; Special syntax to define binned parameters
(Bl, Bh) = binning                      ; => Bl = [1, 2, 3]
                                        ; => Bl = [2, 3, 4]

L matcher =                             ; Specify how keys are matched in this dictionary
  end                                   ;   available: start (default), end, regex, expr
  expr                                  ;   expr => evaluate expression with variable "value"
L empty set = True                      ; How to handle empty dictionary values
                                        ;   False = return single parameter set with empty string
                                        ;    True = return empty parameter set
                                        ;           (this removes the parameter point for cross-products)
L = def                                 ; Define lookup dictionary - default value is L = ['def']
                                        ;   (in this example L is used as "L[KEY1, KEY2]")
  (1, value == '14') => xx              ;   KEY1.endswith('1') && KEY2 == '14'  => L = ['xx']
  (1, ) => "6 7"                        ;   KEY1.endswith('1')                  => L = ['"6 7"']
  (2, ) =>                              ;   KEY1.endswith('2')                  => L = []
  (, int(value) > 4) => 6 7             ;                         int(KEY2) > 4 => L = ['6', '7']

; ------------------------
; SimpleParameterFactory
;   this parameter factory allows to easily describe complex parameter spaces
parameters = A B * C                    ; Parameter space given by: A x B x C (Size: |A|*|B|*|C|)
parameters = (T1, T2)                   ; Parameter space given by the tuples (T1, T2) (Size: min(|T1|, |T2|))
parameters = A D L[A,D]                 ; Parameter space given by: A x D (Size: |A|*|D|)
;                                         ------------------------
parameters = <data>                     ; Explicitely references dataset - redundant, since datasets are
                                        ; automatically added as cross-product if defined but not referenced
parameters = (<data> * A) + 2*B <data>  ; Process dataset with every possible value of A once and
                                        ; process dataset with every possible value of B twice
;                                         ------------------------
parameters = (<data> + <csv1>) <pfs2>   ; Parameter space given by chain of dataset and csv1, crossed with pfs2
pfs2 type = csv                         ; Specify type of parameter reference
                                        ; Available options: dataset, csv
                                        ; Default with dataset definition: dataset - without: csv
pfs2 source = parameters.csv            ; Specify the file the parameters are taken from.
                                        ; The header specifies the variable names
pfs2 format = excel-tab                 ; Default is to guess the csv format from the file itself
                                        ; Available options: [sniffed], excel, excel-tab
csv1 source = pfile1.csv                ;

; ------------------------
; ModularParameterFactory
;   this parameter factory gives complete access to all available parameter sources
;   at the expense of a very verbose parameter space description
parameters = var('A')
parameters = zip(var('T1'), var('T2'))
parameters = cross(var('A'), var('D'), lookup(var('L'), zip(key('A'), key('B'))))

; List of available functions
;  var('X') - access parameter values of variable X
;  key('X') - refer to variable X (in lookups)
;  cross(p1, p2, p3) - compute the cross-product formed from plugin p1, p2 and p3
;  zip(p1, p2, p3) - compute the tuple formed from plugin p1, p2 and p3
;  chain(p1, p2) - chain parameter values together
;  rng('X') - define random variable X
;  const('X', 'VALUE') - define constant X
;  repeat(p1, 5) - repeat parameters given by p1 5 times
;  data('data') - access previously data source with name 'data'
;  counter('X') - create counting variable X (based on job number)
;  format('X', '%04d', 'Y') - format value of Y according to '%04d' and put into X
;  variation(pA, pB, pC) - error variation (Size: 1 + (|A|-1) + (|B|-1) + (|C|-1))
;  ... there are many more which are not yet documented
